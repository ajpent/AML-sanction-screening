{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "import jellyfish #mit\n",
    "import re\n",
    "import numpy as np\n",
    "import rapidfuzz #mit\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First read client data from excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id                    name                    name_parts\n",
      "0           1   MERJA-MAARIT PERHONEN     [MERJA, MAARIT, PERHONEN]\n",
      "1           2           HEIKO PISTOOL              [HEIKO, PISTOOL]\n",
      "2           3    JAANA-MAIJA UUSPELTO      [JAANA, MAIJA, UUSPELTO]\n",
      "3           4       NATNICHA MITJONEN          [NATNICHA, MITJONEN]\n",
      "4           5           KATHRIN FLYKT              [KATHRIN, FLYKT]\n",
      "...       ...                     ...                           ...\n",
      "50016   50017        ALEXINA SARENIUS           [ALEXINA, SARENIUS]\n",
      "50017   50018        ABDISAMED NAZARI           [ABDISAMED, NAZARI]\n",
      "50018   50019            SONER JUUDIN               [SONER, JUUDIN]\n",
      "50019   50020          PIETRO JUURELA             [PIETRO, JUURELA]\n",
      "50020   50021  ABD AL-KHALIQ AL-HUTHI  [ABD, AL, KHALIQ, AL, HUTHI]\n",
      "\n",
      "[50021 rows x 3 columns]\n",
      "elapsed time in reading client data: 3.356520891189575\n"
     ]
    }
   ],
   "source": [
    "program_start = time.time()\n",
    "start = time.time()\n",
    "path = 'C:/Users/Aleksi/Downloads/example_names.xlsx'\n",
    "#Read data in from excel file\n",
    "client_data = pd.read_excel(path)\n",
    "#divide into first and last name for further calculations\n",
    "client_data['name'] = client_data['name'].str.upper()\n",
    "client_data['name_parts'] = client_data['name'].str.split(' |-')\n",
    "print(client_data)\n",
    "end = time.time()\n",
    "elapsed_time = end-start\n",
    "print('elapsed time in reading client data:',elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the xlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aleksi\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ALIAS_NAME CITY CITY_OF_BIRTH CITY_OF_ISSUE  \\\n",
      "0                  NaN           NaN           NaN   \n",
      "1   Jang Chang Ha  NaN           NaN           NaN   \n",
      "2  Jo Chun Ryong   NaN           NaN           NaN   \n",
      "3                  NaN           NaN           NaN   \n",
      "4   Cho Yong Chol  NaN           NaN           NaN   \n",
      "\n",
      "                                           COMMENTS1              COUNTRY  \\\n",
      "0  Ri Won Ho is a DPRK Ministry of State Security...                 None   \n",
      "1                                               None                 None   \n",
      "2                                               None                 None   \n",
      "3  Senior member of Islamic State in Iraq and the...  Trinidad and Tobago   \n",
      "4  Jo Yong Chol is a DPRK Ministry of State Secur...                 None   \n",
      "\n",
      "  COUNTRY_OF_BIRTH COUNTRY_OF_ISSUE   DATAID        DATE  ... TO_YEAR  \\\n",
      "0              NaN              NaN  6908555  1964-07-17  ...     NaN   \n",
      "1              NaN              NaN  6908570  1964-01-10  ...     NaN   \n",
      "2              NaN              NaN  6908571  1960-04-04  ...     NaN   \n",
      "3              NaN              NaN  6908858  1967-07-04  ...     NaN   \n",
      "4              NaN              NaN  6908565  1973-09-30  ...     NaN   \n",
      "\n",
      "  TYPE_OF_DATE                TYPE_OF_DOCUMENT TYPE_OF_DOCUMENT2 UN_LIST_TYPE  \\\n",
      "0        EXACT                        Passport               NaN         DPRK   \n",
      "1        EXACT                             NaN               NaN         DPRK   \n",
      "2        EXACT                             NaN               NaN         DPRK   \n",
      "3        EXACT  National Identification Number               NaN     Al-Qaida   \n",
      "4        EXACT                             NaN               NaN         DPRK   \n",
      "\n",
      "        VALUE VERSIONNUM YEAR ZIP_CODE         Full Name  \n",
      "0        None          1  NaN      NaN        RI  WON HO  \n",
      "1        None          1  NaN      NaN    CHANG CHANG HA  \n",
      "2        None          1  NaN      NaN  CHO   CHUN RYONG  \n",
      "3  2022-04-01          1  NaN      NaN        EMRAAN ALI  \n",
      "4        None          1  NaN      NaN     JO  YONG CHOL  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "elapsed time in reading xlm: 14.570248126983643\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "xml_url = \"https://scsanctions.un.org/resources/xml/en/consolidated.xml\"\n",
    "response = urllib.request.urlopen(xml_url)\n",
    "tree = ET.parse(response)\n",
    "\n",
    "df_sanctioned_individuals = pd.DataFrame()\n",
    "#iterate through all individuals in sanctionlist\n",
    "for individual in tree.findall(\".//INDIVIDUAL\"):\n",
    "    features = {}\n",
    "    #save all the data\n",
    "    for node in individual.iter():\n",
    "        features[node.tag] = node.text\n",
    "    #only saves the last alias without this piece of code\n",
    "    alias_names = []\n",
    "    for alias in individual.findall('.//INDIVIDUAL_ALIAS'):\n",
    "        if alias.find('QUALITY').text == 'Good':\n",
    "            alias_names.append(alias.find('ALIAS_NAME').text)\n",
    "    features['ALIAS_NAME'] = ', '.join(alias_names)\n",
    "    \n",
    "    df_features = pd.DataFrame(features, index=['0'])\n",
    "    df_sanctioned_individuals = df_sanctioned_individuals.append(df_features, ignore_index=True)\n",
    "df_sanctioned_individuals['Full Name'] = df_sanctioned_individuals[['FIRST_NAME', 'SECOND_NAME', 'THIRD_NAME', 'FOURTH_NAME']].fillna('').apply(lambda x: ' '.join(x), axis=1)\n",
    "#this removes the spaces from the end and begginnig\n",
    "df_sanctioned_individuals['Full Name'] = df_sanctioned_individuals['Full Name'].str.strip()\n",
    "#create csv file of the dataframe\n",
    "df_sanctioned_individuals.to_csv('UN_sanction_data.csv', index=False)\n",
    "\n",
    "print(df_sanctioned_individuals[:5])\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print('elapsed time in reading xlm:', elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We only display first 10 hits to save space\n",
      " name                 Full Name                      \n",
      "BI SIDI SOULEMAN     BI SIDI  SOULEMAN                  100.000000\n",
      "MOHAMMAD SAND        MOHAMMAD ZAHID                      81.481483\n",
      "KIM MUN COL          KIM MUN CHOL                        95.652176\n",
      "MARIAL CHANUONG      MARIAL CHANUONG YOL MANGOK         100.000000\n",
      "GHITA AHMED MOHAMED  MOHAMED BEN AHMED MAHRI             81.250000\n",
      "SIN AL-AZZAWI        HIKMAT MIZBAN IBRAHIM AL-AZZAWI     81.818184\n",
      "YAHYA HAQQANI        YAHYA HAQQANI                      100.000000\n",
      "MOHAMMAD SAHARI      MOHAMMAD AHMADI                     86.666664\n",
      "                     MOHAMMAD ZAHID                      82.758621\n",
      "KADRIYE MOHAMED ALI  ALI MOHAMED RAGE                    81.481483\n",
      "dtype: float32\n",
      "We only display first 10 hits to save space\n",
      " Series([], dtype: float32)\n",
      "elapsed time in token set algorithm: 105.08374691009521\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#matches tokens first then compares the nearest tokens with indel algortihm (levenstein but change cost 2(delete + insert))\n",
    "#running with cdist makes it 3 times faster with set of 50 000\n",
    "distances_names = rapidfuzz.process.cdist(client_data['name'], df_sanctioned_individuals['Full Name'], scorer = rapidfuzz.fuzz.token_set_ratio, workers =-1, score_hint=75)\n",
    "distances_aliases = rapidfuzz.process.cdist(client_data['name'], df_sanctioned_individuals['ALIAS_NAME'], scorer = rapidfuzz.fuzz.token_set_ratio, workers =-1)\n",
    "#save results into dataframe\n",
    "distances_names_df = pd.DataFrame(distances_names, index=client_data['name'], columns=df_sanctioned_individuals['Full Name'])\n",
    "distances_aliases_df = pd.DataFrame(distances_aliases, index=client_data['name'], columns=df_sanctioned_individuals['ALIAS_NAME'])\n",
    "#filter with score over 80\n",
    "name_scores_over_80 = distances_names_df[distances_names_df > 80].stack()\n",
    "alias_scores_over_80 = distances_aliases_df[distances_aliases_df > 80].stack()\n",
    "\n",
    "print(\"We only display first 10 hits to save space\\n\", name_scores_over_80[:10])\n",
    "print(\"We only display first 10 hits to save space\\n\", alias_scores_over_80[:10])\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print('elapsed time in token set algorithm:', elapsed_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate soundex for all the sanctioned individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               name_parts name_parts_soundex\n",
      "0         [RI, , WON, HO]    R000  W500 H000\n",
      "1      [CHANG, CHANG, HA]     C520 C520 H000\n",
      "2  [CHO, , , CHUN, RYONG]   C000   C500 R520\n",
      "                  name_parts name_parts_soundex\n",
      "0  [MERJA, MAARIT, PERHONEN]     M620 M630 P655\n",
      "1           [HEIKO, PISTOOL]          H200 P234\n",
      "2   [JAANA, MAIJA, UUSPELTO]     J500 M200 U214\n",
      "elapsed time in transforming names into soundex: 0.4252147674560547\n"
     ]
    }
   ],
   "source": [
    "start = end = time.time()\n",
    "# sanctioned individuals\n",
    "df_sanctioned_individuals['name_parts'] = df_sanctioned_individuals['Full Name'].apply(lambda x: re.split(' |-', x))\n",
    "df_sanctioned_individuals['name_parts_soundex'] = df_sanctioned_individuals['name_parts'].apply(lambda x: ' '.join([jellyfish.soundex(name) for name in x]) if len(x) > 1 else '')\n",
    "\n",
    "print(df_sanctioned_individuals[['name_parts', 'name_parts_soundex']].iloc[0:3])\n",
    "\n",
    "# users\n",
    "client_data['name_parts'] = client_data['name'].apply(lambda x: re.split(' |-', x))    \n",
    "client_data['name_parts_soundex'] = client_data['name_parts'].apply(lambda x: ' '.join(jellyfish.soundex(name) for name in x))\n",
    "print(client_data[['name_parts', 'name_parts_soundex']].iloc[0:3])\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print('elapsed time in transforming names into soundex:', elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soundex compare UN list with our list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                    Full Name                               \n",
      "BI SIDI SOULEMAN        BI SIDI  SOULEMAN                           100.0\n",
      "ANDRUS ANTEROINEN       ANDERS CAMEROON OSTENSVIG DALE              100.0\n",
      "MACO ALA-OLLA           ALI MAYCHOU                                 100.0\n",
      "BAYAN SUITTIO           SA'D BIN SA'D MUHAMMAD SHARIYAN AL-KA'BI    100.0\n",
      "                        MAHRI SIDI AMAR BEN DAHA                    100.0\n",
      "                                                                    ...  \n",
      "MEHMET TAPIO            ALLAH DAD TAYEB WALI MUHAMMAD               100.0\n",
      "ROSALIA REKOLA          MOHAMMAD RASUL AYYUB                        100.0\n",
      "HAMED KURRU             HAMADA OULD MOHAMED EL KHAIRY               100.0\n",
      "TEIJU TOHKA             QARI SAIFULLAH TOKHI                        100.0\n",
      "ABD AL-KHALIQ AL-HUTHI  ABD AL-KHALIQ  AL-HOUTHI                    100.0\n",
      "Length: 771, dtype: float32\n",
      "elapsed time in token set algorithm for soundex: 47.352813482284546\n"
     ]
    }
   ],
   "source": [
    "start = end = time.time()\n",
    "#matches tokens first then compares the nearest tokens with indel algortihm (levenstein but change cost 2(delete + insert))\n",
    "distances = rapidfuzz.process.cdist(client_data['name_parts_soundex'], df_sanctioned_individuals['name_parts_soundex'], scorer = rapidfuzz.fuzz.token_set_ratio, workers =-1, score_hint=100)\n",
    "\n",
    "scores_100_soundex = pd.DataFrame(distances, index=client_data['name'], columns=df_sanctioned_individuals['Full Name'])\n",
    "scores_100_soundex = scores_100_soundex[scores_100_soundex == 100].stack()\n",
    "\n",
    "print(scores_100_soundex)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print('elapsed time in token set algorithm for soundex:', elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time for whole progam: 171.19136667251587\n"
     ]
    }
   ],
   "source": [
    "writer = pd.ExcelWriter('SanctionScreenResults.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# write each dataframe to a separate worksheet\n",
    "name_scores_over_80.to_excel(writer, sheet_name='Name hits token set ratio')\n",
    "#alias_scores_over_80.to_excel(writer, sheet_name='Alias hits token set ratio')\n",
    "scores_100_soundex.to_excel(writer, sheet_name='Soundex Results')\n",
    "# save the writer\n",
    "writer.save()\n",
    "\n",
    "program_end = time.time()\n",
    "elapsed_program_time = program_end-program_start\n",
    "print('elapsed time for whole progam:',elapsed_program_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function for reading a single name in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We only display first 10 hits to save space\n",
      "                   Full Name        \n",
      "BI SIDI SOULEMAN  BI SIDI  SOULEMAN    100.0\n",
      "dtype: float32\n",
      "We only display first 10 hits to save space\n",
      " Series([], dtype: float32)\n",
      "elapsed time in token set algorithm: 0.6109671592712402\n"
     ]
    }
   ],
   "source": [
    "def calculate_single(name):\n",
    "    start = time.time()\n",
    "    #matches tokens first then compares the nearest tokens with indel algortihm (levenstein but change cost 2(delete + insert))\n",
    "    #running with cdist makes it 3 times faster with set of 50 000\n",
    "    distances_names = rapidfuzz.process.cdist([name], df_sanctioned_individuals['Full Name'], scorer = rapidfuzz.fuzz.token_set_ratio, workers =-1, score_hint=75)\n",
    "    distances_aliases = rapidfuzz.process.cdist([name], df_sanctioned_individuals['ALIAS_NAME'], scorer = rapidfuzz.fuzz.token_set_ratio, workers =-1)\n",
    "    #save results into dataframe\n",
    "    distances_names_df = pd.DataFrame(distances_names, index=[name], columns=df_sanctioned_individuals['Full Name'])\n",
    "    distances_aliases_df = pd.DataFrame(distances_aliases, index=[name], columns=df_sanctioned_individuals['ALIAS_NAME'])\n",
    "    #filter with score over 80\n",
    "    name_scores_over_80 = distances_names_df[distances_names_df > 80].stack()\n",
    "    alias_scores_over_80 = distances_aliases_df[distances_aliases_df > 80].stack()\n",
    "\n",
    "    print(\"We only display first 10 hits to save space\\n\", name_scores_over_80[:10])\n",
    "    print(\"We only display first 10 hits to save space\\n\", alias_scores_over_80[:10])\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    print('elapsed time in token set algorithm:', elapsed_time)\n",
    "    return (name_scores_over_80,alias_scores_over_80)\n",
    "\n",
    "\n",
    "df_namescore = pd.DataFrame()\n",
    "df_alias_score = pd.DataFrame()\n",
    "df_namescore,df_alias_score=calculate_single(\"BI SIDI SOULEMAN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slower version but can be used to double check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start = end = time.time()\n",
    "# Define a function to calculate the distance between a client and a sanction list suspect\n",
    "def calculate_distance(client, suspect_list):\n",
    "    #distances = np.vectorize(rapidfuzz.fuzz.token_sort_ratio)(client, suspect_list)\n",
    "    distances = np.vectorize(rapidfuzz.fuzz.token_set_ratio)(client, suspect_list)\n",
    "    max_distance = distances.max()\n",
    "    max_suspect = suspect_list[distances.argmax()]\n",
    "    return max_distance, max_suspect\n",
    "# Apply the function to each row of the client_data DataFrame\n",
    "client_data['max_distance_token_set'] = 0\n",
    "client_data['max_suspect_token_set'] = ''\n",
    "client_data['max_distance_token_set'], client_data['max_suspect_token_set'] = zip(*client_data['name'].apply(lambda x: calculate_distance(x, df_sanctioned_individuals['Full Name'])))\n",
    "\n",
    "filtered_data_token_set = client_data[client_data['max_distance_token_set']>80]\n",
    "print(filtered_data_token_set[['name','max_suspect_token_set', 'max_distance_token_set']])\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print('elapsed time in token set algorithm:', elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
